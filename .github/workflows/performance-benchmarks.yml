name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance benchmarks weekly
    - cron: '0 3 * * 1'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - extended
          - stress
          - comparison

env:
  R_REMOTES_NO_ERRORS_FROM_WARNINGS: true

jobs:
  # Core performance benchmarks
  core-benchmarks:
    runs-on: ubuntu-latest
    name: Core Performance Benchmarks

    steps:
      - uses: actions/checkout@v4

      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Setup dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: |
            any::bench
            any::microbenchmark
            any::profvis
            any::pryr
            any::tictoc
            local::.
          needs: check

      - name: Database performance benchmarks
        run: |
          library(bench)
          library(microbenchmark)
          library(zzedc)

          cat("üèÉ Database Performance Benchmarks\n")
          cat("=================================\n\n")

          # Benchmark 1: Database Connection Performance
          cat("1. Database Connection Benchmarks:\n")
          db_benchmark <- bench::mark(
            "create_pool" = {
              pool <- pool::dbPool(RSQLite::SQLite(), dbname = ":memory:",
                                 minSize = 1, maxSize = 5)
              pool::poolClose(pool)
            },
            "get_connection" = {
              pool <- pool::dbPool(RSQLite::SQLite(), dbname = ":memory:",
                                 minSize = 1, maxSize = 5)
              conn <- pool::poolCheckout(pool)
              pool::poolReturn(conn)
              pool::poolClose(pool)
            },
            "single_connection" = {
              conn <- RSQLite::dbConnect(RSQLite::SQLite(), ":memory:")
              RSQLite::dbDisconnect(conn)
            },
            iterations = 50,
            check = FALSE
          )

          print(db_benchmark)
          autoplot(db_benchmark)
          ggsave("db_connection_benchmark.png", width = 10, height = 6)

          # Performance thresholds
          median_pool_time <- median(db_benchmark[db_benchmark$expression == "create_pool", ]$median)
          if (median_pool_time > 0.01) {  # 10ms threshold
            cat("‚ö†Ô∏è  Database pool creation slower than expected:", median_pool_time, "seconds\n")
          } else {
            cat("‚úÖ Database pool performance within limits\n")
          }

          # Benchmark 2: Data Operations
          cat("\n2. Data Operation Benchmarks:\n")

          # Create test database
          pool <- pool::dbPool(RSQLite::SQLite(), dbname = ":memory:")
          pool::dbExecute(pool, "
            CREATE TABLE test_subjects (
              subject_id TEXT PRIMARY KEY,
              age INTEGER,
              enrollment_date DATE,
              status TEXT
            )
          ")

          # Insert test data
          test_data <- data.frame(
            subject_id = paste0("SUBJ", sprintf("%04d", 1:1000)),
            age = sample(18:80, 1000, replace = TRUE),
            enrollment_date = as.Date("2024-01-01") + sample(0:365, 1000, replace = TRUE),
            status = sample(c("Active", "Completed", "Withdrawn"), 1000, replace = TRUE)
          )

          data_ops_benchmark <- microbenchmark(
            "insert_single" = {
              pool::dbExecute(pool,
                "INSERT INTO test_subjects VALUES (?, ?, ?, ?)",
                params = list("TEST_SINGLE", 25, as.Date("2024-01-01"), "Active")
              )
            },
            "insert_batch_10" = {
              batch_data <- test_data[1:10, ]
              pool::dbWriteTable(pool, "test_subjects", batch_data, append = TRUE)
            },
            "select_all" = {
              pool::dbGetQuery(pool, "SELECT * FROM test_subjects LIMIT 100")
            },
            "select_filtered" = {
              pool::dbGetQuery(pool, "SELECT * FROM test_subjects WHERE age > ? AND status = ?",
                             params = list(30, "Active"))
            },
            "count_query" = {
              pool::dbGetQuery(pool, "SELECT COUNT(*) FROM test_subjects")
            },
            times = 100
          )

          print(data_ops_benchmark)
          pool::poolClose(pool)

          cat("‚úÖ Database operation benchmarks completed\n")
        shell: Rscript {0}

      - name: Application startup benchmarks
        run: |
          library(tictoc)
          library(pryr)
          library(zzedc)

          cat("\nüöÄ Application Startup Benchmarks\n")
          cat("==================================\n\n")

          # Benchmark application startup components
          startup_times <- list()

          # 1. Package loading time
          tic("Package loading")
          library(shiny)
          library(bslib)
          library(DT)
          library(ggplot2)
          startup_times$package_loading <- toc(quiet = TRUE)

          # 2. Database setup time
          tic("Database setup")
          if (file.exists("data/memory001_study.db")) {
            file.remove("data/memory001_study.db")
          }
          # Don't actually run setup_database() as it's heavy, just measure connection
          pool <- pool::dbPool(RSQLite::SQLite(), dbname = ":memory:")
          pool::poolClose(pool)
          startup_times$database_setup <- toc(quiet = TRUE)

          # 3. UI compilation time
          tic("UI compilation")
          # Simulate UI creation without actually starting server
          tryCatch({
            # Create minimal UI structure
            ui_elements <- fluidPage(
              titlePanel("Test"),
              sidebarLayout(
                sidebarPanel("Sidebar"),
                mainPanel("Main")
              )
            )
          }, error = function(e) {
            cat("UI compilation error:", e$message, "\n")
          })
          startup_times$ui_compilation <- toc(quiet = TRUE)

          # Memory usage analysis
          cat("\nüíæ Memory Usage Analysis:\n")
          mem_usage <- pryr::mem_used()
          cat("Current memory usage:", mem_usage, "\n")

          # Report startup performance
          cat("\nüìä Startup Performance Summary:\n")
          for (component in names(startup_times)) {
            time_taken <- startup_times[[component]]$toc - startup_times[[component]]$tic
            cat(sprintf("%-20s: %.3f seconds\n", component, time_taken))

            # Check against thresholds
            if (time_taken > 2.0) {
              cat("‚ö†Ô∏è ", component, "is slower than expected\n")
            }
          }

          cat("‚úÖ Application startup benchmarks completed\n")
        shell: Rscript {0}

      - name: Data processing benchmarks
        run: |
          library(bench)
          library(microbenchmark)

          cat("\nüìä Data Processing Benchmarks\n")
          cat("=============================\n\n")

          # Generate test datasets of different sizes
          small_data <- data.frame(
            id = 1:100,
            value = rnorm(100),
            category = sample(LETTERS[1:5], 100, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:30, 100, replace = TRUE)
          )

          medium_data <- data.frame(
            id = 1:10000,
            value = rnorm(10000),
            category = sample(LETTERS[1:5], 10000, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:365, 10000, replace = TRUE)
          )

          large_data <- data.frame(
            id = 1:100000,
            value = rnorm(100000),
            category = sample(LETTERS[1:5], 100000, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:365, 100000, replace = TRUE)
          )

          # Benchmark data processing operations
          cat("1. Data Filtering Performance:\n")
          filter_benchmark <- microbenchmark(
            "small_filter" = {
              filtered <- small_data[small_data$value > 0 & small_data$category == "A", ]
              nrow(filtered)
            },
            "medium_filter" = {
              filtered <- medium_data[medium_data$value > 0 & medium_data$category == "A", ]
              nrow(filtered)
            },
            "large_filter" = {
              filtered <- large_data[large_data$value > 0 & large_data$category == "A", ]
              nrow(filtered)
            },
            times = 50
          )

          print(filter_benchmark)

          cat("\n2. Data Aggregation Performance:\n")
          agg_benchmark <- microbenchmark(
            "small_aggregate" = {
              agg <- aggregate(value ~ category, data = small_data, FUN = mean)
              nrow(agg)
            },
            "medium_aggregate" = {
              agg <- aggregate(value ~ category, data = medium_data, FUN = mean)
              nrow(agg)
            },
            "large_aggregate" = {
              agg <- aggregate(value ~ category, data = large_data, FUN = mean)
              nrow(agg)
            },
            times = 30
          )

          print(agg_benchmark)

          cat("\n3. Data Validation Performance:\n")
          validation_benchmark <- microbenchmark(
            "date_validation" = {
              valid_dates <- !is.na(medium_data$date) &
                           medium_data$date >= as.Date("2024-01-01") &
                           medium_data$date <= as.Date("2024-12-31")
              sum(valid_dates)
            },
            "numeric_validation" = {
              valid_numbers <- !is.na(medium_data$value) &
                             is.finite(medium_data$value) &
                             medium_data$value >= -10 &
                             medium_data$value <= 10
              sum(valid_numbers)
            },
            "categorical_validation" = {
              valid_categories <- medium_data$category %in% LETTERS[1:5]
              sum(valid_categories)
            },
            times = 100
          )

          print(validation_benchmark)

          # Performance thresholds
          large_filter_time <- median(filter_benchmark[filter_benchmark$expr == "large_filter", ]$time) / 1e9
          if (large_filter_time > 0.1) {  # 100ms threshold for 100k records
            cat("‚ö†Ô∏è  Large data filtering slower than expected:", large_filter_time, "seconds\n")
          } else {
            cat("‚úÖ Data filtering performance within limits\n")
          }

          cat("‚úÖ Data processing benchmarks completed\n")
        shell: Rscript {0}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks
          path: |
            *.png
            *.csv
            *.txt
        if: always()

  # Memory and resource benchmarks
  resource-benchmarks:
    runs-on: ubuntu-latest
    name: Resource Usage Benchmarks

    steps:
      - uses: actions/checkout@v4

      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Setup dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: |
            any::bench
            any::pryr
            any::profvis
            local::.

      - name: Memory usage benchmarks
        run: |
          library(pryr)
          library(bench)

          cat("üíæ Memory Usage Benchmarks\n")
          cat("==========================\n\n")

          # Memory usage analysis
          initial_memory <- pryr::mem_used()
          cat("Initial memory usage:", initial_memory, "\n")

          # Test memory usage with different data sizes
          memory_tests <- list()

          # Small dataset
          cat("\n1. Small dataset memory usage:\n")
          gc() # Garbage collect first
          mem_before <- pryr::mem_used()

          small_dataset <- data.frame(
            id = 1:1000,
            value1 = rnorm(1000),
            value2 = sample(letters, 1000, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:30, 1000, replace = TRUE)
          )

          mem_after <- pryr::mem_used()
          memory_tests$small <- mem_after - mem_before
          cat("Memory used for 1K records:", memory_tests$small, "\n")

          # Medium dataset
          cat("\n2. Medium dataset memory usage:\n")
          rm(small_dataset)
          gc()
          mem_before <- pryr::mem_used()

          medium_dataset <- data.frame(
            id = 1:10000,
            value1 = rnorm(10000),
            value2 = sample(letters, 10000, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:365, 10000, replace = TRUE)
          )

          mem_after <- pryr::mem_used()
          memory_tests$medium <- mem_after - mem_before
          cat("Memory used for 10K records:", memory_tests$medium, "\n")

          # Large dataset
          cat("\n3. Large dataset memory usage:\n")
          rm(medium_dataset)
          gc()
          mem_before <- pryr::mem_used()

          large_dataset <- data.frame(
            id = 1:100000,
            value1 = rnorm(100000),
            value2 = sample(letters, 100000, replace = TRUE),
            date = as.Date("2024-01-01") + sample(0:365, 100000, replace = TRUE)
          )

          mem_after <- pryr::mem_used()
          memory_tests$large <- mem_after - mem_before
          cat("Memory used for 100K records:", memory_tests$large, "\n")

          # Memory efficiency analysis
          cat("\nüìà Memory Efficiency Analysis:\n")
          small_per_record <- as.numeric(memory_tests$small) / 1000
          medium_per_record <- as.numeric(memory_tests$medium) / 10000
          large_per_record <- as.numeric(memory_tests$large) / 100000

          cat(sprintf("Memory per record (1K):   %.2f bytes\n", small_per_record))
          cat(sprintf("Memory per record (10K):  %.2f bytes\n", medium_per_record))
          cat(sprintf("Memory per record (100K): %.2f bytes\n", large_per_record))

          # Check for memory efficiency degradation
          if (large_per_record > small_per_record * 1.5) {
            cat("‚ö†Ô∏è  Memory efficiency degrades with larger datasets\n")
          } else {
            cat("‚úÖ Memory usage scales efficiently\n")
          }

          # Cleanup
          rm(large_dataset)
          gc()

          cat("‚úÖ Memory benchmarks completed\n")
        shell: Rscript {0}

      - name: Database connection stress test
        if: github.event.inputs.benchmark_type == 'stress' || github.event.inputs.benchmark_type == 'extended'
        run: |
          library(pool)
          library(bench)

          cat("\nüîó Database Connection Stress Test\n")
          cat("=================================\n\n")

          # Test connection pool under load
          cat("1. Connection pool stress test:\n")

          stress_test <- function(n_connections = 10, n_operations = 100) {
            pool <- pool::dbPool(RSQLite::SQLite(), dbname = ":memory:",
                               minSize = 2, maxSize = n_connections)

            # Create test table
            pool::dbExecute(pool, "
              CREATE TABLE stress_test (
                id INTEGER PRIMARY KEY,
                value REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
              )
            ")

            start_time <- Sys.time()

            # Simulate concurrent operations
            for (i in 1:n_operations) {
              tryCatch({
                # Insert operation
                pool::dbExecute(pool,
                  "INSERT INTO stress_test (value) VALUES (?)",
                  params = list(runif(1))
                )

                # Select operation
                if (i %% 10 == 0) {
                  result <- pool::dbGetQuery(pool, "SELECT COUNT(*) FROM stress_test")
                }
              }, error = function(e) {
                cat("Error in operation", i, ":", e$message, "\n")
              })
            }

            end_time <- Sys.time()
            duration <- as.numeric(end_time - start_time)

            # Get final count
            final_count <- pool::dbGetQuery(pool, "SELECT COUNT(*) as count FROM stress_test")
            pool::poolClose(pool)

            list(
              duration = duration,
              operations = n_operations,
              success_rate = final_count$count / n_operations,
              ops_per_second = n_operations / duration
            )
          }

          # Run stress tests with different loads
          stress_results <- list(
            light = stress_test(5, 50),
            medium = stress_test(10, 200),
            heavy = stress_test(20, 500)
          )

          cat("Stress Test Results:\n")
          for (test_name in names(stress_results)) {
            result <- stress_results[[test_name]]
            cat(sprintf("%-10s: %.2f ops/sec, %.1f%% success, %.2f sec total\n",
                      test_name,
                      result$ops_per_second,
                      result$success_rate * 100,
                      result$duration))
          }

          # Performance thresholds
          if (stress_results$heavy$ops_per_second < 50) {
            cat("‚ö†Ô∏è  Database performance under heavy load is below threshold\n")
          } else {
            cat("‚úÖ Database handles stress test successfully\n")
          }

          cat("‚úÖ Connection stress test completed\n")
        shell: Rscript {0}

  # Generate performance report
  performance-report:
    needs: [core-benchmarks, resource-benchmarks]
    if: always()
    runs-on: ubuntu-latest
    name: Performance Report

    steps:
      - name: Generate performance summary
        run: |
          echo "## ‚ö° ZZedc Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Status" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark Suite | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|----------------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Core Benchmarks | ${{ needs.core-benchmarks.result }} | Database, startup, data processing |" >> $GITHUB_STEP_SUMMARY
          echo "| Resource Benchmarks | ${{ needs.resource-benchmarks.result }} | Memory usage, connection stress |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- üîó **Database Operations**: Connection pooling, CRUD operations" >> $GITHUB_STEP_SUMMARY
          echo "- üöÄ **Application Startup**: Package loading, UI compilation" >> $GITHUB_STEP_SUMMARY
          echo "- üìä **Data Processing**: Filtering, aggregation, validation" >> $GITHUB_STEP_SUMMARY
          echo "- üíæ **Memory Usage**: Efficiency analysis across dataset sizes" >> $GITHUB_STEP_SUMMARY
          echo "- üîó **Connection Stress**: Multi-user simulation testing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Standards" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Database connections < 10ms" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Data filtering (100K records) < 100ms" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Memory scaling efficiency maintained" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Connection pool handles 20+ concurrent users" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Application startup < 2 seconds per component" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìà Continuous Monitoring" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmarks run weekly on main branch" >> $GITHUB_STEP_SUMMARY
          echo "- Performance regression detection" >> $GITHUB_STEP_SUMMARY
          echo "- Resource usage tracking over time" >> $GITHUB_STEP_SUMMARY
          echo "- Stress testing for scalability validation" >> $GITHUB_STEP_SUMMARY